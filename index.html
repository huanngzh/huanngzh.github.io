<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta property="og:locale" content="en">
    <meta property="og:site_name" content="Zehuan Huang">
    <meta property="og:title" content="Zehuan Huang">
    <meta name="description" content="Home page of Zehuan Huang">
    <link rel="stylesheet" href="./css/jemdoc.css" type="text/css">
    <link rel="stylesheet" href="./css/custom.css" type="text/css">
    <link rel="icon" type="image/x-icon" href="./files/favicon.ico">
    <link rel="shortcut icon" type="image/x-icon" href="./files/favicon.ico">

    <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
    <link rel="manifest" href="images/site.webmanifest">

    <title>Zehuan Huang - Homepage</title>
</head>


<body>

    <div id="layout-content" style="margin-top:25px">

        <table>
            <tbody>
                <tr>
                    <td width="700">
                        <div id="toptitle">
                            <h1>Zehuan Huang (é»„æ³½æ¡“)&nbsp;</h1>
                        </div>
                        <h3>Beihang University</h3>
                        <br>
                        <p>
                            Email: <a href="mailto:huanngzh@gmail.com">huanngzh@gmail.com</a> <br>
                            <a href="https://github.com/huanngzh" target="_blank">[Github]</a>
                            <a href="https://scholar.google.com/citations?user=U3VbX6kAAAAJ" target="_blank">[Google
                                Scholar]</a>
                            <a href="https://x.com/huanngzh" target="_blank">[Twitter]</a>
                            <a href="./files/zehuan_cv_en.pdf" target="_blank">[CV]</a> <br>
                        </p>
                    </td>

                    <td><img src="./images/photo.png" border="0" width="200"></td>
                </tr>
            </tbody>
        </table>


        <h2>Biography</h2>
        <p>
            I am a master student in School of Software from Beihang University now, supervised by <a
                href="https://lucassheng.github.io/" target="_blank">Prof. Lu Sheng</a>.
        </p>

        <p>
            My prior research focused on applying deep generative models to 3D asset creation, encompassing the
            generation of 3D objects, scenes, textures, and animations. My current research interests lie in <b>interactive world
                models and simulation</b>, including (i) Simulation-Ready 3D Generation and Reconstruction, (ii) Interactive World Modeling, and (iii) Native Multi-Modal Generative Models.
        </p>

        <p>
            I am grateful to all my collaborators and mentors along the way. I first started doing research under the
            guidance of <a href="https://miaowang.me/" target="_blank">Prof. Miao Wang</a>. Then I started working on
            deep learning related projects under the supervision of <a href="https://lucassheng.github.io/"
                target="_blank">Prof. Lu Sheng</a>. Besides, I also haved intern at <a
                href="https://minimaxi.com/" target="_blank">MiniMax</a>, <a href="https://www.shlab.org.cn/"
                target="_blank">Shanghai AI Lab</a>, <a href="https://www.tripo3d.ai/" target="_blank">VAST</a>, and <a href="https://3d.hunyuan.tencent.com/" target="_blank">Tencent Hunyuan3D</a>, and
            I'm fortunate to have worked closely with <a href="https://jtdong.com/" target="_blank">Junting Dong</a>, <a
                href="https://scholar.google.com/citations?user=b7ZJV9oAAAAJ" target="_blank">Yuan-Chen Guo</a>, <a
                href="https://yanpei.me/" target="_blank">Yanpei Cao</a>, <a href="https://yang-l1.github.io/" target="_blank">Yang Li</a>, Zhuo Chen, and Chunchao Guo.
        </p>

        <!-- <p style="color: crimson;"> -->
        <p>
            I am always open to academic and industrial collaborations, if you share the vision, please do not hesitate
            to contact me!
        </p>

        <h2>News</h2>

        <div style="height: 200px; overflow: auto;">
            <ul>
                <li>[2025-11] One paper <a href="https://fenghora.github.io/Personalize-Anything-Page/" target="_blank">Personalize Anything</a> is accepted by AAAI 2026.</li>
                <li>[2025-11] One paper <a href="https://huanngzh.github.io/VoxHammer-Page/" target="_blank">VoxHammer</a> is accepted by 3DV 2026 as <b>oral presentation</b>.</li>
                <li>[2025-09] One paper <a href="https://anima-x.github.io/"
                    target="_blank">AnimaX</a> is accepted by SIGGRAPH Asia 2025.</li>
                <li>[2025-07] One paper <a href="https://huanngzh.github.io/Parts2Whole/"
                        target="_blank">Parts2Whole</a> is accepted by TIP.</li>
                <li>
                    [2025-06] One paper <a href="https://huanngzh.github.io/MV-Adapter-Page/"
                        target="_blank">MV-Adapter</a> is accepted by ICCV 2025.
                </li>
                <li>
                    [2025-05] Open-source <a href="https://github.com/huanngzh/bpy-renderer"
                        target="_blank">bpy-renderer</a>, a python package for rendering 3D scenes and animations using
                    blender.
                </li>
                <li>
                    [2025-03] Two papers <a href="https://huanngzh.github.io/MIDI-Page/" target="_blank">MIDI-3D</a> and
                    <a href="https://costwen.github.io/Ouroboros3D/" target="_blank">Ouroboros3D</a> are accepted by
                    CVPR 2025.
                </li>
                <li>
                    [2024-07] One paper <a href="https://jtdong.com/tela_layer/" target="_blank">TELA</a> is accepted by
                    ECCV 2024.
                </li>
                <li>
                    [2024-03] Invited talk at <a href="https://anysyn3d.github.io/" target="_blank">AnySyn3D</a> on
                    Compositional 3D Scene Generation (<a href="https://www.bilibili.com/video/BV1gZZNYQErL/"
                        target="_blank">video</a>).
                </li>
                <li>
                    [2024-02] One paper <a href="https://huanngzh.github.io/EpiDiff/" target="_blank">EpiDiff</a> is
                    accepted by CVPR 2024.
                </li>
            </ul>
        </div>

        <!-- <h2>Selected Preprints</h2>

        <table id="tbPublications" width="100%" cellspacing="0">
            <tbody>
                <tr style="width: 100%;" class="publication-highlight">
                    <td width="20%" class="publication-img">
                        <div class="container">
                            <img src="./files/teasers/mvadapter.png" width="180px"
                                style="box-shadow: 4px 4px 4px #888888; margin-left: 10px;">
                        </div>
                    </td>
                    <td class="publication-text">
                        <div class="publication-info">
                            <div class="publication-title">MV-Adapter: Multi-view Consistent Image Generation Made Easy
                            </div>
                            <div class="publication-authors"><b>Zehuan Huang</b>, Yuan-Chen Guo, Haoran Wang, Ran Yi,
                                Lizhuang Ma, Yan-Pei Cao<sup>âœ‰</sup>, Lu Sheng<sup>âœ‰</sup></div>
                            <div class="publication-venue">Under Review</div>
                            <div class="publication-links">
                                <div class="links-container">
                                    <a href="https://huanngzh.github.io/MV-Adapter-Page/" target="_blank">[Project
                                        Page]</a> /
                                    <a href="https://arxiv.org/pdf/2412.03632" target="_blank">[Paper]</a> /
                                    <a href="https://github.com/huanngzh/MV-Adapter" target="_blank">[Code]</a> /
                                    <a href="https://mp.weixin.qq.com/s/1raVsiSk4CtrU33TdX_wBg" target="_blank">[æœºå™¨ä¹‹å¿ƒ]</a>
                                </div>
                                <a href="https://github.com/huanngzh/MV-Adapter" class="github-stars">
                                    <img src="https://img.shields.io/github/stars/huanngzh/MV-Adapter?style=social"
                                        alt="">
                                </a>
                            </div>
                            <div class="publication-intro"><u>TL;DR:</u> Versatile multi-view generation with various
                                base models and conditions, and high-quality 3D texture generation.
                            </div>
                        </div>
                    </td>
                </tr>
            </tbody>
        </table> -->

        <h2>Selected Publications</h2>

        <div class="section-header">3D Generation</div>
        
        <table id="tbPublications" width="100%" cellspacing="0">
            <tbody>
                <tr style="width: 100%;">
                    <td width="20%" class="publication-img">
                        <div class="container">
                            <img src="./files/teasers/animax.png" width="180px"
                                style="box-shadow: 4px 4px 4px #888888; margin-left: 10px;">
                        </div>
                    </td>
                    <td class="publication-text">
                        <div class="publication-info">
                            <div class="publication-title">AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion Models</div>
                            <div class="publication-authors"><b>Zehuan Huang</b>, Haoran Feng, Yangtian Sun, Yuanchen Guo, Yanpei Cao, Lu Sheng</div>
                            <div class="publication-venue">SIGGRAPH Asia 2025</div>
                            <div class="publication-links">
                                <div class="links-container">
                                    <a href="https://anima-x.github.io/" target="_blank">[Project Page]</a> /
                                    <a href="https://arxiv.org/pdf/2506.19851" target="_blank">[Paper]</a> /
                                    <a href="https://mp.weixin.qq.com/s/C96-KmRbDZl_YWxNigOqMA"
                                        target="_blank">[æœºå™¨ä¹‹å¿ƒ]</a>
                                </div>
                            </div>
                            <div class="publication-intro"><u>TL;DR:</u> Animate any 3D skeleton with joint video-pose diffusion models.</div>
                        </div>
                    </td>
                </tr>

                <tr style="width: 100%;" class="publication-highlight">
                    <td width="20%" class="publication-img">
                        <div class="container">
                            <img src="./files/teasers/mvadapter.png" width="180px"
                                style="box-shadow: 4px 4px 4px #888888; margin-left: 10px;">
                        </div>
                    </td>
                    <td class="publication-text">
                        <div class="publication-info">
                            <div class="publication-title">MV-Adapter: Multi-view Consistent Image Generation Made Easy
                            </div>
                            <div class="publication-authors"><b>Zehuan Huang</b>, Yuan-Chen Guo, Haoran Wang, Ran Yi,
                                Lizhuang Ma, Yan-Pei Cao<sup>âœ‰</sup>, Lu Sheng<sup>âœ‰</sup></div>
                            <div class="publication-venue">ICCV 2025</div>
                            <div class="publication-links">
                                <div class="links-container">
                                    <a href="https://huanngzh.github.io/MV-Adapter-Page/" target="_blank">[Project
                                        Page]</a> /
                                    <a href="https://arxiv.org/pdf/2412.03632" target="_blank">[Paper]</a> /
                                    <a href="https://github.com/huanngzh/MV-Adapter" target="_blank">[Code]</a> /
                                    <a href="https://mp.weixin.qq.com/s/1raVsiSk4CtrU33TdX_wBg"
                                        target="_blank">[æœºå™¨ä¹‹å¿ƒ]</a>
                                </div>
                                <a href="https://github.com/huanngzh/MV-Adapter" class="github-stars">
                                    <img src="https://img.shields.io/github/stars/huanngzh/MV-Adapter?style=social"
                                        alt="">
                                </a>
                            </div>
                            <div class="publication-intro"><u>TL;DR:</u> Versatile multi-view generation with various
                                base models and conditions, and high-quality 3D texture generation.
                            </div>
                        </div>
                    </td>
                </tr>

                <tr style="width: 100%;" class="publication-highlight">
                    <td width="20%" class="publication-img">
                        <div class="container">
                            <img src="./files/teasers/midi.png" width="180px"
                                style="box-shadow: 4px 4px 4px #888888; margin-left: 10px;">
                        </div>
                    </td>
                    <td class="publication-text">
                        <div class="publication-info">
                            <div class="publication-title">MIDI: Multi-Instance Diffusion for Single Image to 3D Scene
                                Generation</div>
                            <div class="publication-authors"><b>Zehuan Huang</b>, Yuan-Chen Guo, Xingqiao An, Yunhan
                                Yang, Yangguang Li, Zi-Xin Zou, Ding Liang, Xihui Liu, Yan-Pei Cao<sup>âœ‰</sup>, Lu
                                Sheng<sup>âœ‰</sup></div>
                            <div class="publication-venue">CVPR 2025</div>
                            <div class="publication-links">
                                <div class="links-container">
                                    <a href="https://huanngzh.github.io/MIDI-Page/" target="_blank">[Project Page]</a> /
                                    <a href="https://arxiv.org/pdf/2412.03558" target="_blank">[Paper]</a> /
                                    <a href="https://github.com/VAST-AI-Research/MIDI-3D" target="_blank">[Code]</a> /
                                    <a href="https://mp.weixin.qq.com/s/PE3JUGsHKjFWEiCJMg9tHQ"
                                        target="_blank">[æœºå™¨ä¹‹å¿ƒ]</a>
                                </div>
                                <a href="https://github.com/VAST-AI-Research/MIDI-3D" class="github-stars">
                                    <img src="https://img.shields.io/github/stars/VAST-AI-Research/MIDI-3D?style=social"
                                        alt="">
                                </a>
                            </div>
                            <div class="publication-intro"><u>TL;DR:</u> MIDI-3D extends image-to-3D object generation
                                models to multi-instance diffusion models for compositional 3D scene generation.</div>
                        </div>
                    </td>
                </tr>

            </tbody>
        </table>

        <br>
        <i style="font-size: 13px"><b>*</b> Equal contribution. <b>â€ </b> Project Lead. <b>âœ‰</b>
            Corresponding author.</i>

        <h2>Selected Leading Projects</h2>

        <!-- <div class="section-header">3D Generation</div> -->
        
        <table id="tbPublications" width="100%" cellspacing="0">
            <tbody>
                <tr style="width: 100%;" class="">
                    <td width="20%" class="publication-img">
                        <div class="container">
                            <img src="./files/teasers/voxhammer.png" width="180px"
                                style="box-shadow: 4px 4px 4px #888888; margin-left: 10px;">
                        </div>
                    </td>
                    <td class="publication-text">
                        <div class="publication-info">
                            <div class="publication-title">VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space
                            </div>
                            <div class="publication-authors">Lin Li*, <b>Zehuan
                                Huang</b>*â€ , Haoran Feng, Gengxiong Zhuang, Rui Chen, Chunchao Guo, Lu Sheng<sup>âœ‰</sup>
                            <div class="publication-venue">3DV 2026 Oral</div>
                            <div class="publication-links">
                                <div class="links-container">
                                    <a href="https://huanngzh.github.io/VoxHammer-Page/" target="_blank">[Project
                                        Page]</a> /
                                    <a href="https://arxiv.org/pdf/2508.19247" target="_blank">[Paper]</a> /
                                    <a href="https://github.com/Nelipot-Lee/VoxHammer" target="_blank">[Code]</a>
                                </div>
                                <a href="https://github.com/Nelipot-Lee/VoxHammer" class="github-stars">
                                    <img src="https://img.shields.io/github/stars/Nelipot-Lee/VoxHammer?style=social"
                                        alt="">
                                </a>
                            </div>
                            <div class="publication-intro"><u>TL;DR:</u> A training-free 3D editing approach that performs precise and coherent editing in native 3D latent space instead of multi-view space.
                            </div>
                        </div>
                    </td>
                </tr>
            </tbody>
        </table>

        <h2>Open-Source Projects</h2>

        <div class="project-grid">
            <div class="project-card">
                <div class="project-header">
                    <a href="https://github.com/huanngzh/ComfyUI-MVAdapter" target="_blank" class="project-title">
                        ðŸŽ¨ ComfyUI-MVAdapter
                    </a>
                    <div class="project-stars">
                        <a href="https://github.com/huanngzh/ComfyUI-MVAdapter" class="github-stars">
                            <img src="https://img.shields.io/github/stars/huanngzh/ComfyUI-MVAdapter?style=social"
                                alt="">
                        </a>
                    </div>
                </div>
                <div class="project-description">
                    Custom nodes for using MV-Adapter for multi-view synthesis in ComfyUI.
                </div>
                <div class="project-tech">
                    <span class="tech-tag">ComfyUI</span>
                    <span class="tech-tag">PyTorch</span>
                    <span class="tech-tag">Diffusion</span>
                </div>
            </div>

            <div class="project-card">
                <div class="project-header">
                    <a href="https://github.com/huanngzh/bpy-renderer" target="_blank" class="project-title">
                        ðŸŽ¨ bpy-renderer
                    </a>
                    <div class="project-stars">
                        <a href="https://github.com/huanngzh/bpy-renderer" class="github-stars">
                            <img src="https://img.shields.io/github/stars/huanngzh/bpy-renderer?style=social" alt="">
                        </a>
                    </div>
                </div>
                <div class="project-description">
                    Python package for rendering 3D scenes and animations using blender.
                </div>
                <div class="project-tech">
                    <span class="tech-tag">Blender</span>
                    <span class="tech-tag">Python</span>
                    <span class="tech-tag">3D Rendering</span>
                </div>
            </div>
        </div>

        <h2>Selected Honors & Awards</h2>
        <div style="height: 200px; overflow: auto;">
            <ul>
                <li>
                    [2025] <b>Shen Yuan Modal</b> (highest honor for students in Beihang University)
                </li>
                <li>
                    [2025] <b>National Scholarship</b>
                </li>
                <li>
                    [2025] Outstanding Student Award, Beihang University
                </li>
                <li>
                    [2024] <b>National Scholarship</b>
                </li>
                <li>
                    [2024] Outstanding Student Award, Beihang University
                </li>
                <li>
                    [2023] <b>Outstanding Graduate Student of Beijing</b>
                </li>
                <li>
                    [2022] Outstanding Student Award, Beihang University
                </li>
            </ul>
        </div>

        <h2>Educations</h2>
        <div>
            <ul>
                <li>
                    Sept. 2023 - Jan. 2026, Master, Beihang University, Beijing. Supervised by <a
                        href="https://lucassheng.github.io/" target="_blank">Prof. Lu Sheng</a>.
                </li>
                <li>
                    Sept. 2019 - June 2023, Bachelor, Beihang University, Beijing.
                </li>
            </ul>
        </div>

        <h2>Industrial Experience</h2>
        <div>
            <ul>
                <li>
                    June. 2025 - Jan. 2026, <a href="https://3d.hunyuan.tencent.com/" target="_blank">Tencent Hunyuan</a>, Beijing. Work on
                    unified multi-modal models for 3D.
                </li>
                <li>
                    Dec. 2023 - May 2025, <a href="https://www.tripo3d.ai/" target="_blank">VAST</a>, Beijing. Work on
                    3D texture and scene generation.
                </li>
                <li>
                    Aug. 2023 - Dec. 2023, <a href="https://www.shlab.org.cn/" target="_blank">Shanghai Artificial
                        Intelligence Laboratory</a>, Beijing. Work on object-level multi-view generation.
                </li>
                <li>
                    May 2022 - June 2023, <a href="https://minimaxi.com/" target="_blank">MiniMax</a>, Beijing. Work on
                    3D avatar reconstruction, controllable image generation.
                </li>
            </ul>
        </div>


        <h2>Services</h2>
        <h3>Reviewer</h3>
        ICLR, NeurIPS, CVPR, ICCV, ACM MM, AAAI, IJCV, TCSVT

        <h3>In-School</h3>
        2023 Fall ~ 2025 Spring, part-time technology counselor in School of Software, Beihang University<br>
        2024 Spring, TA in Image Processing and Computer Vision, instructed by <a href="https://lucassheng.github.io/"
            target="_blank">Prof. Lu Sheng</a><br>
    </div>

    <div id="footer">
        <div id="footer-text"></div>
    </div>
    &copy 2026 Zehuan Huang


    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-4HYX9V4BH0"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', "G-4HYX9V4BH0");
    </script>
</body>

</html>